---
title: "Machine Learning 2019: Hw4"
author: "Tayler Sindhu"
date: November 4, 2019
output: html_document
---

## Homework

1. Compare the most important features from at least 2 different classes of feature selection methods covered in this tutorial with any reasonable machine learning dataset from mlbench. Do these feature selection methods provide similar results? 

2. Attempt a feature selection method not covered in this tutorial (backward elimination, forward propogation, etc.)

```{r setup, include=FALSE}
# Loading libraries
library(mlbench)
library(caret)
library(randomForest)
```

### **Question 1**

```{r Question 1, load data}
# load data
data(Glass)

# Check variable types
str(Glass)

# Confirm that there are no Na values
sum(is.na(Glass))

# Change outcome variables from numbers to letters to avoid errors in algorithms
levels(Glass[,10]) <- c("one", "two", "three", "five", "six", "seven")
```


```{r Wrapper method, Question 1}
# Question 1
# wrapper method

# define control 
control = rfeControl(functions = caretFuncs, number = 2)

# recursive feature elimination algorithm
results <- rfe(Glass[,1:9], Glass[,10], sizes = c(2,5,6,9), rfeControl = control, method = "svmRadial")

results
results$variables
```

```{r Embedded method, Question 1}
# random forest

# make training and test sets
train_size <- floor(0.75 * nrow(Glass))
set.seed(15)
train_pos <- sample(seq_len(nrow(Glass)), size = train_size)

train_classification <- Glass[train_pos, ]
test_classification <- Glass[-train_pos, ]

#fit model
rfmodel = randomForest(Type ~ ., data=train_classification,  importance = TRUE, oob.times = 15, confusion = TRUE)

#rank features based on importance 
importance(rfmodel, type=2)

# visualizing feature importance 
varImpPlot(rfmodel, type=2, main = "Variable Importance Plot")
```

#### **Summary**

#### Overall, these methods produced similar results. Random forest selected Mg, Al, RI, Ca and Na to be the top 5 most important variables, while recursive feature elimination chose the top five variables to be Mg, Al, Na, K, and Ca. Only the variables RI and K did not overlap; however, K was the sixth most important variable according to the random forest model. 

### **Question 2**

```{r Filter method, warning=FALSE}
# Filter selection with univariate filters

# Set training and test sets
set.seed(10)
train_size_sbf <- floor(0.75 * nrow(Glass))
train_pos_sbf <- sample(seq_len(nrow(Glass)), size = train_size_sbf)

train_classification_sbf <- Glass[train_pos_sbf, ]
test_classification_sbf <- Glass[-train_pos_sbf, ]

# make control

sbf_ctrl <- sbfControl(functions = caretSBF, method = "cv")

# make model
results_sbf <- sbf(train_classification_sbf[,1:9], train_classification_sbf$Type, sbfControl = sbf_ctrl)

# Display chosen variable 

results_sbf
results_sbf$variables
```

Overall, the filtering method had similar results. This method identified Na, Mg, Al, K, and Ba as the most important features in each iteration of cross validation, which has considerable overlap with the other methods of feature selection (excluding Ba in both other methods, and K in recursive feature elimination only). This method also frequently included Ca, which was identified in the other two methods as an important variable.

